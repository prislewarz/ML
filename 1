import tensorflow as ft

# X and Y data
x_train = [1,2,3]
y_train = [1.2.3]

W = tf.variable(tf.random_normal([1]), name='weight')
b = tf.variable(tf.random_normal([1]), name='bias')

#Our hypothesis XW+b
hypothesis = x_train*W + b

#Cost, loss function
cost = tf.reduce_mean(tf.square(hypothesis - y_train))

#minimize
optimizer = tf.train.GardienDescentOptimizer(learning_rate=0.01)
train = optimizer.minimize(cost)

#Launch the gragh in a session
sess = tf.session()
#Initializes global variable in the gragh
sess.run(tf.global_variable_initializer())

#Fit the line
for step in range(2001):
    sess.run(train)
    if step % 20 == 0:
        print(step, sess.run(cost), sess.run(W), sess.run(b))

텐서 플로우2는 케라스를 사용하기 때문에 텐서 플로우 1보다 코드가 많이 짧아졌다.

선형 회귀를 위해서 텐서 플로우 1에서는 다음과 같은 코드를 만들어 텐서를 구성해야 했다.

W = tf.Variable(tf.random_normal([1), name = 'weight') B = tf.Variable(tf.random_normal([1), name = 'bias') x = tf.placeholder(tf.flat32, shape = [None] y = tf.placeholder(tf.flat32, shape = [None] 다음으로 가설을 구성해야 했다.

hypothesis = x * w + b 손실 함수를 구성해야 한다.

cost = tf.reduce_mean(tf.squre(hypothesis - y)) 경사 하강법을 사용하기 위해서 Optimizer를 구성해야 했다.

GradientDecentOptimizer() 텐서 플로우 2에서는 아래와 같이 사용한다.

keras는 기본적으로 다차원 레이어를 기본으로 하고 있기 때문에 Sequentail()을 만들어 생성

keras의 다차원 계층 모델인 Sequential를 레이어를 만든다.
model = tf.keras.models.Sequential()

입력이 1차원이고 출력이 1차원임을 뜻함 - Dense는 레이어의 종류
model.add(tf.keras.layers.Dense(1, input_dim = 1)

Optimizer - Stochastic gradient descent - 확률적 경사 하강법
sgd = tf.keras.optimizers.SGD(learning_rate=0.01)

cost/loss funcion
loss를 mean_squared_error 방식을 사용한다는 의미로 mse 라고 써도 인식한다.
model.compile(loss='mean_squared_error',optimizer=sgd)

fit the line
텐서 플로우 1과 다르게 세션을 만들어서 돌릴 필요가 없다.
간단하게 만들어서 학습을 시작한다.
model.fit(x_train,y_train,epochs=2000) 학습한 모델에 데이터를 넣어보면 다음과 같이 나온다.

x 값이 5면 예상값이 얼마인지 확인하는 부분
print(model.predict(np.array([5]))) => [[4.9985623]]

이 코드는 줄이 20미만인 경우에 해당되는 full code임

