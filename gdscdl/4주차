import torch
m = torch.nn.Dropout(p=0.5)
input = torch.randn(4, 4)
output = m(input)

print(input)
print(output)

m = torch.nn.BatchNorm1d(5)
input = torch.randn(5, 5)
output = m(input)

print(input)
print(output)
print(output.mean(axis=0),output.var(axis=0))

# 재료 준비
import torch
# device 설정 (gpu & cpu)
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# random seed 고정 (항상 같은 결과값이 나오게 하기 위함)
torch. manual_seed(777)
if device == 'cuda':
  torch.cuda.manual_seed_all(777)
  
x_train = torch.FloatTensor([[1, -0.3, 0.8],
                             [0.7, 0.2, 0.6],
                             [0.3, 0.5, 0.3],
                             [1, 1, 1.1],
                             [-1, -1.5, -1.2]]).to(device)
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]]).to(device)
  
# 실제로 모델을 만드는 부분
model = torch.nn.Linear(3, 1).to(device) 

# loss function과 optimizer 선언
criterion = torch.nn.MSELoss().to(device)
optimizer = torch.optim.SGD(model.parameters(), lr=1e-1,weight_decay=1)

for epoch in range(20):
    hypothesis = model(x_train) # 예측값 뽑아내기
    cost = criterion(hypothesis, y_train) # loss 계산
    optimizer.zero_grad() # optimizer에 저장된 gradient값을 0으로 초기화 
    cost.backward() # loss를 back propagation 연산
    optimizer.step() # back propatation의 결과를 바탕으로 optimizer를 이용해 weight 업데이트

    
    print(epoch, cost.item())
    
import torch
from torch import nn, optim
from torch.utils.data import TensorDataset, DataLoader
# 데이터를 훈련용과 테스트용으로 분리
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from tqdm import tqdm
from torchsummary import summary
import matplotlib.pyplot as plt

# 전체의 20%는 검증용
digits = load_digits()
X = digits.data
Y = digits.target
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)

X_train = torch.tensor(X_train, dtype=torch.float32).to(device)
Y_train = torch.tensor(Y_train, dtype=torch.int64).to(device)
X_test = torch.tensor(X_test, dtype=torch.float32).to(device)
Y_test = torch.tensor(Y_test, dtype=torch.int64).to(device)

linear1 = torch.nn.Linear(64, 100, bias=True)
linear2 = torch.nn.Linear(100, 100, bias=True)
linear3 = torch.nn.Linear(100, 100, bias=True)
linear4 = torch.nn.Linear(100, 100, bias=True)
linear5 = torch.nn.Linear(100, 10, bias=True)
relu = torch.nn.ReLU()
dropout = torch.nn.Dropout(0.5)
batch = torch.nn.BatchNorm1d(100)

model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3,relu,
                           linear4,relu,linear5).to(device)
                           
lossFunc = nn.CrossEntropyLoss()

optimizer = optim.Adam(model.parameters())

ds = TensorDataset(X_train, Y_train)
loader = DataLoader(ds, batch_size=32, shuffle=True)

trainLosses = []
testLosses = []

for epoch in tqdm(range(100)):
    runningLoss = 0.0
    # 신경망을 train 모드로 설정
    model.train()
    for i, (batchX, batchY) in enumerate(loader):
        optimizer.zero_grad()
        
        yPred = model(batchX)
        loss = lossFunc(yPred, batchY)
        loss.backward()
        optimizer.step()
        runningLoss += loss.item()
    trainLosses.append(runningLoss/i)
    
    # 신경망을 test 모드로 설정
    model.eval()
    yPred = model(X_test)
    testLoss = lossFunc(yPred, Y_test)
    testLosses.append(testLoss.item())

plt.plot(range(100), trainLosses, label = "train loss")
plt.plot(range(100), testLosses, label = "test loss")
plt.legend()

model = torch.nn.Sequential(linear1, relu, dropout,linear2, relu,dropout, linear3, relu,dropout,
                           linear4,relu,dropout,linear5).to(device)
                           
summary(model.to(device), (64,))

lossFunc = nn.CrossEntropyLoss()

optimizer = optim.Adam(model.parameters())

ds = TensorDataset(X_train, Y_train)
loader = DataLoader(ds, batch_size=32, shuffle=True)

trainLosses = []
testLosses = []

for epoch in tqdm(range(100)):
    runningLoss = 0.0
    # 신경망을 train 모드로 설정
    model.train()
    for i, (batchX, batchY) in enumerate(loader):
        optimizer.zero_grad()
        
        yPred = model(batchX)
        loss = lossFunc(yPred, batchY)
        loss.backward()
        optimizer.step()
        runningLoss += loss.item()
    trainLosses.append(runningLoss/i)
    
    # 신경망을 test 모드로 설정
    model.eval()
    yPred = model(X_test)
    testLoss = lossFunc(yPred, Y_test)
    testLosses.append(testLoss.item())

plt.plot(range(100), trainLosses, label = "train loss")
plt.plot(range(100), testLosses, label = "test loss")
plt.legend()

model = torch.nn.Sequential(linear1, relu, batch,dropout,linear2, relu ,batch,dropout, linear3, relu,batch,dropout,linear4,relu, batch, dropout,linear5).to(device)
summary(model.to(device), (64,))

lossFunc = nn.CrossEntropyLoss()

optimizer = optim.Adam(model.parameters())

ds = TensorDataset(X_train, Y_train)
loader = DataLoader(ds, batch_size=32, shuffle=True)

trainLosses = []
testLosses = []

for epoch in tqdm(range(100)):
    runningLoss = 0.0
    # 신경망을 train 모드로 설정
    model.train()
    for i, (batchX, batchY) in enumerate(loader):
        optimizer.zero_grad()
        
        yPred = model(batchX)
        loss = lossFunc(yPred, batchY)
        loss.backward()
        optimizer.step()
        runningLoss += loss.item()
    trainLosses.append(runningLoss/i)
    
    # 신경망을 test 모드로 설정
    model.eval()
    yPred = model(X_test)
    testLoss = lossFunc(yPred, Y_test)
    testLosses.append(testLoss.item())

plt.plot(range(100), trainLosses, label = "train loss")
plt.plot(range(100), testLosses, label = "test loss")
plt.legend()

torch.nn.init.xavier_uniform_(linear1.weight)
torch.nn.init.xavier_uniform_(linear2.weight)
torch.nn.init.xavier_uniform_(linear3.weight)
torch.nn.init.xavier_uniform_(linear4.weight)
torch.nn.init.xavier_uniform_(linear5.weight)

model = torch.nn.Sequential(linear1, relu, batch,dropout,linear2, relu ,batch,dropout, linear3, relu,batch,dropout,linear4,relu, batch, dropout,linear5).to(device)

lossFunc = nn.CrossEntropyLoss()

optimizer = optim.Adam(model.parameters())

ds = TensorDataset(X_train, Y_train)
loader = DataLoader(ds, batch_size=32, shuffle=True)

trainLosses = []
testLosses = []

for epoch in tqdm(range(100)):
    runningLoss = 0.0
    # 신경망을 train 모드로 설정
    model.train()
    for i, (batchX, batchY) in enumerate(loader):
        optimizer.zero_grad()
        
        yPred = model(batchX)
        loss = lossFunc(yPred, batchY)
        loss.backward()
        optimizer.step()
        runningLoss += loss.item()
    trainLosses.append(runningLoss/i)
    
    # 신경망을 test 모드로 설정
    model.eval()
    yPred = model(X_test)
    testLoss = lossFunc(yPred, Y_test)
    testLosses.append(testLoss.item())

plt.plot(range(100), trainLosses, label = "train loss")
plt.plot(range(100), testLosses, label = "test loss")
plt.legend()
