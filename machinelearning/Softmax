#다중 분류 신경망 구현
def sigmoid(self, z):
  a=1/(1+np.exp(-z)) #시그모이드 계산
  return a

def softmax(self,z):
  exp_z=np.exp(z) #소프트맥스 함수
  return exp_z/np.sum(exp_z, axis=1).reshape(-1,1) 

def init_weights(self, n_features, n_classes):
  self.w2=np.random.normal(0,1,(self.units, n_classes)) #은닉층의 크기, 클래스의 개수
  self.b2=np.zeros(n_classes)

def upate_val_loss(self, x_val, y_val):
  a=self.softmax(z)
  val_loss=np.sum(-y_val * np.log(a)) #크로스 엔트로피 손실과 규제 손실을 더하여 리스트에 추가 
  
크로스 엔트로피 손실(Cross Entropy Loss)은 머신 러닝의 분류 모델이 얼마나 잘 수행되는지 측정하기 위해 사용되는 지표입니다. 
Loss(또는 Error)는0은 완벽한 모델로0과 1 사이의 숫자로 측정됩니다. 일반적인 목표는 모델을 가능한 0에 가깝게 만드는 것입니다.
딥러닝에서는 실제 데이터의 확률 분포와, 학습된 모델이 계산한 확률 분포의 차이를 구하는데 사용된다.

#의류 분류 모델
(x_train_all, y_train_all), (x_test, y_test)= tf.keras.datasets.fashion_mnist.load_data()
print(x_train_all.shape, y_train_all.shape)
import matplotlib.pyplot as plt
plt.imshow(x_train_all[0], cmap='gray')
plt.show(
#타깃 확인
print(y_train_all[:10])
class_names=['티셔츠','바지''등등']
print(class_names[y_train_all[0]]))
np.bincount(y_train_all)
#훈련세트와 검증세트 준비
from sklearn.model_selection import train_test_split
x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all, stratify=y_train_all, test_size=0.2, random_state=42)
np.bincount(y_train)
np.bincount(y_val)
x_train = x_train / 255
x_val = x_val / 255
#g훈련세트와 검증세트 차원 변경
x_train=x_train.reshape(-1,784)
x_val=x_val.reshape(-1,784)
print(x_train.shape, x_val.shape)
#to_categorical 함수로 원-핫 인코딩
tf.keras.utils.to_categorical([0,1,3])
y_train_encoded = tf.keras.utils.to_categorical(y_train)
y_val=encoded = tf.keras.utils.to_categorical(y_val)
print(y_train_encoded.shape, y_val_encoded.shape)
print(y_train[0], y_train_encoded[0 ])

#Sequential 모델 훈련하기
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
model = Sequential()
model.add(Dense(100, activation-'sigmoid', input_shape=(794,)))
model.add(Dense(10, activation='softmax'))
model.complie(optimizer='sgd', loss='categorical_crossentropy', metrics=['accurasy'])
history = model.fit(x_train, y_train_encoded, epochs=40, validation_data=(x_val, y_val_encoded))

원-핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식입니다.

